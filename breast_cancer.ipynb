{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11071739",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "In this exercise, I will ask you to use the famous \"Breast Cancer Wisconsin\" dataset from Scikit-learn. The goal is to build classification models to predict whether a tumor is malignant or benign based on various features.\n",
    "\n",
    "**Dataset Description:**\n",
    "The Breast Cancer Wisconsin dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The features describe various characteristics of cell nuclei present in the image. The target variable is binary, where '0' represents malignant tumors, and '1' represents benign tumors. Here you can find more details: https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "**Step 1: Data Loading**\n",
    "1. Load the Breast Cancer Wisconsin dataset from Scikit-learn using `sklearn.datasets.load_breast_cancer()`.\n",
    "2. Split the data into features and target variables.\n",
    "\n",
    "**Step 2: Data Preprocessing**\n",
    "3. Split the dataset into a training set and a testing set (e.g., 70% train and 30% test).\n",
    "4. Perform any necessary data preprocessing and feature engineering, such as scaling the features.\n",
    "\n",
    "**Step 2.5: Feature selection**\n",
    "5. Apply initial feature selection process (e.g., use some statisical tests like Fisher)\n",
    "\n",
    "**Step 3** Baseline model\n",
    "6. Please create simple logistic regression model as a baseline.\n",
    "\n",
    "**Step 4: AdaBoost Classifier**\n",
    "7. Train an AdaBoost classifier on the training data.\n",
    "8. Use cross-validation to find the optimal number of base estimators (n_estimators) for AdaBoost.\n",
    "9. Tune other hyperparameters (e.g., learning rate) using cross-validation.\n",
    "10. Visualize the feature importances in the model and try to apply additional feature selection based on it.\n",
    "11. Evaluate the model's performance on the test set using accuracy, precision-recall curve, and F1-score.\n",
    "\n",
    "**Step 5: Gradient Boosting Machine (GBM)**\n",
    "12. Train a Gradient Boosting Machine classifier on the training data.\n",
    "13. Use cross-validation to find the optimal values for hyperparameters like the number of trees (n_estimators), maximum depth (max_depth), and learning rate.\n",
    "14. Visualize the feature importances in the model and try to apply additional feature selection based on it.\n",
    "15. Evaluate the GBM model's performance on the test set using accuracy, precision-recall curve, and F1-score.\n",
    "\n",
    "**Step 6: Model Comparison and **\n",
    "16. Compare the performance of the AdaBoost and GBM classifiers and Logistc Regression.\n",
    "17. Summarize the results and provide insights on which algorithm performed better on this dataset and why.\n",
    "18. Discuss the impact of hyperparameter tuning on model performance.\n",
    "\n",
    "Hint, here you will find a case study from Machine Learning 1 where we discuss the entire model creation pipeline using various feature engineering and feature selection techniques: https://github.com/michaelwozniak/ML-in-Finance-I-case-study-forecasting-tax-avoidance-rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1284c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_breast_cancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ec18e",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b2de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer Wisconsin dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969d80",
   "metadata": {},
   "source": [
    "# Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7bcf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set and a testing set (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f781cd5",
   "metadata": {},
   "source": [
    "# Step 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff7c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select the top k features (you can adjust k as needed)\n",
    "k = 10  # Number of features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e93534",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fcd36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression baseline model: 96.49%\n"
     ]
    }
   ],
   "source": [
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the Logistic Regression baseline model: {:.2f}%\".format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
